## Directory Structure Introduction
``` 
├── AddcdivKernelInvocation
│   ├── cmake                   // Compilation project files
│   ├── input                   // Directory for storing input data generated by scripts
│   ├── output                  // Directory for storing operator runtime output data and ground truth data
│   ├── scripts
│   │   ├── acl.json            // acl configuration file
│   │   ├── gen_data.py         // Script for generating input data and ground truth data
│   │   └── verify_result.py    // Ground truth comparison file
│   ├── addcdiv_custom.cpp      // Operator kernel implementation
│   ├── CMakeLists.txt          // Compilation project file
│   ├── data_utils.h            // Data input and output functions
│   ├── main.cpp                // Main function, application that invokes the operator, including CPU and NPU domain calls
│   └── run.sh                  // Script for compiling and running the operator
``` 

## Code Implementation Introduction
This invocation example implements the Addcdiv operator with a fixed shape of 8*2048.

- Kernel Implementation   
  The mathematical expression for the Addcdiv operator is:  
  ```
  out = (x + (y / z) * value)
  ```
  The calculation logic is: The vector calculation interfaces provided by Ascend C operate on LocalTensor elements. Input data needs to be first moved into on-chip storage, then the calculation interfaces are used to divide the two input parameters y and z, multiply the result by the attribute value value, add this result to the input parameter x, and finally move the result out to external storage.   

  The implementation process of the Addcdiv operator is divided into three basic tasks: CopyIn, Compute, and CopyOut. The CopyIn task is responsible for moving the input Tensor xGm, yGm, and zGm from Global Memory to Local Memory, stored in xLocal, yLocal, and zLocal respectively. The Compute task is responsible for performing operations on xLocal, yLocal, and zLocal, and storing the result in outLocal. The CopyOut task is responsible for moving the output data from outLocal to the output Tensor outGm in Global Memory. For details, please refer to [addcdiv_custom.cpp](./addcdiv_custom.cpp).

- Invocation Implementation  
  1. CPU-side runtime verification is mainly completed through interfaces provided by the CPU debugging library such as the ICPU_RUN_KF CPU debugging macro;  
  2. NPU-side runtime verification is mainly completed using the <<<>>> kernel invocation operator.    
  The application distinguishes between code logic running on the CPU side and the NPU side through the ASCENDC_CPU_DEBUG macro.

## Running the Example Operator
- Open the example directory

  ```bash
  cd ${git_clone_path}/samples/operator_contrib/AddcdivCustomSample/KernelLaunch/AddcdivKernelInvocation
  ```

- Configure environment variables

  Please select the corresponding command to configure environment variables based on the [installation method](https://hiascend.com/document/redirect/CannCommunityInstSoftware) of the CANN development toolkit package on the current environment.
    - Default path, root user installs CANN software package
      ```bash
      export ASCEND_INSTALL_PATH=/usr/local/Ascend/ascend-toolkit/latest
      ```
    - Default path, non-root user installs CANN software package
      ```bash
      export ASCEND_INSTALL_PATH=$HOME/Ascend/ascend-toolkit/latest
      ```
    - Specified path install_path, installs CANN software package
      ```bash
      export ASCEND_INSTALL_PATH=${install_path}/ascend-toolkit/latest
      ```
    
    Configure the simulation mode log file directory, default is sim_log.
    ```bash
    export CAMODEL_LOG_PATH=./sim_log
    ```

- Example execution

  ```bash
  bash run.sh -r [RUN_MODE] -v  [SOC_VERSION] 
  ```
  - RUN_MODE: Compilation method, can choose CPU debugging, NPU simulation, NPU on-board. Supported parameters are [cpu / sim / npu], default is cpu.
  - SOC_VERSION: Ascend AI processor model. If you are unsure of the specific [SOC_VERSION], execute the npu-smi info command on the server with the Ascend AI processor to query it. Add the Ascend information before the "Name" in the query result. For example, if the "Name" corresponds to the value xxxyy, the actual configured [SOC_VERSION] value is Ascendxxxyy. Supported parameter values (replace xxx with the specific value):
    - Atlas inference series products (Ascend 310P processor) parameter values: Ascend310P1, Ascend310P3
    - Atlas training series products parameter values: AscendxxxA, AscendxxxB
    - Atlas A2 training series products parameter values: AscendxxxB1, AscendxxxB2, AscendxxxB3, AscendxxxB4

  Note: For Atlas training series products using NPU simulation debugging, there may be precision issues. You can choose other chips for NPU simulation debugging.

  Example:
  ```bash
  bash run.sh -r cpu -v Ascend310P1
  ```   

## Update Log
  | Date       | Update Items |
  |------------|--------------|
  | 2023/5/24  | Added this readme |
  | 2023/7/24  | Modified readme format |